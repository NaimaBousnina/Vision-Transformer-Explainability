## Introduction

Modern deep learning solutions often leverage transformer architectures, extending their application to computer vision tasks, notably in face recognition. Those techniques are called Vision Transformers (ViT). Although they show a certain degree of explainability through their self-attention mechanism, which propagation-based tools can leverage to generate attention maps, these maps provide insights solely into critical regions in a classification context rather than a face verification context.
Therefore, to enhance the interpretability of face verification decisions, a pivotal strategy involves merging the probe and gallery attention maps generated by a ViT propagation-based tool. The core objective is to identify the shared crucial regions between the attention maps, leading to the generation of a face verification explainability heatmap. This explainability method is versatile, capable of explaining both positive and negative decisions made by the model.

## Requirements

To run this project, make sure you have the following dependencies installed:

- [OpenCV-Python](https://pypi.org/project/opencv-python/) ~=4.5.2.54
- [MXNet](https://pypi.org/project/mxnet/) ~=1.8.0.post0
- [Matplotlib](https://pypi.org/project/matplotlib/) ~=3.4.2
- [Scikit-learn](https://pypi.org/project/scikit-learn/) ~=0.24.2
- [IPython](https://pypi.org/project/ipython/) ~=7.24.1
- [ViT PyTorch](https://pypi.org/project/vit-pytorch/)
- [TIMM](https://pypi.org/project/timm/) ==0.3.2
- [PyTorch](https://pypi.org/project/torch/) ~=1.9.0
- [Torchvision](https://pypi.org/project/torchvision/) ~=0.10.0
- [NumPy](https://pypi.org/project/numpy/) ~=1.21.0
- [Pillow](https://pypi.org/project/Pillow/) ~=8.2.0
- [SciPy](https://pypi.org/project/scipy/) ~=1.7.0
- [Einops](https://pypi.org/project/einops/) ~=0.3.0


## Datasets

The training database is MS-Celeb-1M (version [ms1m-retinaface](https://github.com/deepinsight/insightface/tree/master/challenges/iccv19-lfr)).

### Testing Datasets

#### LFW (Labelled Faces in the Wild) 

The Labeled Faces in the Wild (LFW) dataset is a widely used benchmark in the field of face recognition. It was created by researchers at the University of Massachusetts, and it was introduced to evaluate the performance of face recognition algorithms in unconstrained real-world scenarios.

#### SLLFW (Similar Looking LFW)

The SLLFW dataset addresses a limitation in the original LFW database by introducing 3000 similar-looking impostor pairs. This aims to provide a more realistic scenario for face verification where impostors can resemble genuine users.

#### CALFW (Cross-Age LFW)

The CALFW dataset emphasizes age gaps within genuine face pairs. It includes 3000 genuine pairs with significant age differences, enlarging the intra-class variant within genuine pairs while carefully selecting impostor pairs to avoid different gender and race.

#### CPLFW (Cross-Pose LFW)

CPLFW focuses on pose intra-class variation within genuine verification pairs. It comprises 3000 genuine face pairs with pose differences, along with impostor pairs having the same gender and race. The CPLFW dataset exhibits large intra-class variance and small inter-class variance.

#### TALFW (Transformed LFW)

TALFW introduces imperceptible noise to the original LFW dataset. Using a greedy algorithm, a minimum number of candidate face images were selected, resulting in 4069 faces images with subtle modifications amounting to only 1.34 in terms of the root mean squared deviation.

#### CFP-FP (Celebrities in Frontal-Profile)

The CFP-FP dataset explores challenges posed by extreme pose variations. It contains 10 frontal and 4 profile images of 500 individuals, with each of the 10 splits consisting of 350 positive and 350 negative pairs.

#### AgeDB-30 (AgeDataBase-30)

The AgeDB-30 dataset comprises 16,488 images of famous individuals with a wide range of ages. On average, each person is 50.3 years old. The dataset is divided into 10 folds for evaluation, with each fold comprising 300 positive and 300 negative pairs.


## Train ViT Model

o train the model, you can use the following bash script that covers multiple scenarios:

```bash
# Training Scenario 1
CUDA_VISIBLE_DEVICES='0,1,2,3' python3 -u train_GVIT.py -b 480 -w 0,1,2,3 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_ms1m_cosface_s1 --warmup-epochs 1 --lr 3e-4

# Training Scenario 2
CUDA_VISIBLE_DEVICES='0,1,2,3' python3 -u train_GVIT.py -b 480 -w 0,1,2,3 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_ms1m_cosface_s2 --warmup-epochs 0 --lr 1e-4 -r path_to_model

# Training Scenario 3
CUDA_VISIBLE_DEVICES='0,1,2,3' python3 -u train_GVIT.py -b 480 -w 0,1,2,3 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_ms1m_cosface_s3 --warmup-epochs 0 --lr 5e-5 -r path_to_model
```

## Test ViT Model and Explainability Tool

To evaluate the model and run the explainability tool on different datasets, you can use the following commands:

```bash
python3 test_GViT.py --target "lfw"

python3 test_GViT.py --target "talfw"
```

## How to cite?
R. Correia, F. Pereira, and P. L. Correia, “Face Verification Explainability Heatmap Generation Using a Vision Transformer,” in 
International Conf. of the Biometrics Special Interest Group - BIOSIG, Germany, Darmstad, Sep. 2023.
